---
title: 近期LLM的部署与应用经历(2)
date: '2025-03-07'
linkTitle: https://mabbs.github.io/2025/03/08/llm2.html
source: .na.character
description: |-
  <p>最近AI发展好快啊～<!--more--></p> <h1 id="起因">起因</h1>
  <p>自从<a href="/2025/02/22/llm.html">上次</a>写完文章之后，最近这段时间LLM圈又有了不少更新，感觉很值得试试看。所以这次就来看看这些新东西有什么特别的地方吧。</p> <h1 id="关于阿里qwq模型的体验">关于阿里QwQ模型的体验</h1>
  <p>前两天阿里的推理模型QwQ模型更新到正式版了，不过其实我也没试过他们的预览版效果怎么样……但按照他们的说法，他们的32b参数的模型水平已经相当于DeepSeek-R1 671b的模型了。如果真是这样，那就太好了，毕竟那个671b参数的模型部署难度还是相当大的，在当时想部署一个能用级别的还是挺烧钱的。但如果这个32b参数的模型能达到相 ...
disable_comments: true
---
<p>最近AI发展好快啊～<!--more--></p> <h1 id="起因">起因</h1>
<p>自从<a href="/2025/02/22/llm.html">上次</a>写完文章之后，最近这段时间LLM圈又有了不少更新，感觉很值得试试看。所以这次就来看看这些新东西有什么特别的地方吧。</p> <h1 id="关于阿里qwq模型的体验">关于阿里QwQ模型的体验</h1>
<p>前两天阿里的推理模型QwQ模型更新到正式版了，不过其实我也没试过他们的预览版效果怎么样……但按照他们的说法，他们的32b参数的模型水平已经相当于DeepSeek-R1 671b的模型了。如果真是这样，那就太好了，毕竟那个671b参数的模型部署难度还是相当大的，在当时想部署一个能用级别的还是挺烧钱的。但如果这个32b参数的模型能达到相 ...